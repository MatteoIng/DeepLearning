{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain MRI Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install d2l==1.0.0a1.post0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from glob import glob\n",
    "from d2l import tensorflow as d2l\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import keras.layers as layer\n",
    "from PIL import Image\n",
    "# For GPU\n",
    "from numba import jit,cuda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione di una specifica struttura del dataset per UNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi serve per salvare il path delle immagini\n",
    "img = []\n",
    "\n",
    "# Percorso per prendere tutte le maschere e non le immagini\n",
    "msk = glob('C:/Users/MATTEO/Desktop/Programmi/Python/DeepLearning/ProgettoDeep/Brain_MRI/*/*_mask*')\n",
    "\n",
    "# Scorro le maschere e rimuovo mask per la corrispondente immagine associata\n",
    "for i in msk:\n",
    "    img.append(i.replace('_mask',''))\n",
    "\n",
    "\n",
    "for i in range(len(msk)):\n",
    "    # sposto immagine\n",
    "    im=Image.open(img[i])\n",
    "    ms=Image.open(msk[i])\n",
    "    im.save(\"C:\\\\Users\\\\MATTEO\\\\Desktop\\\\Programmi\\\\Python\\\\DeepLearning\\\\ProgettoDeep\\\\DsDL\\\\images\\\\prova\"+str(i)+\".tif\")\n",
    "    ms.save(\"C:\\\\Users\\\\MATTEO\\\\Desktop\\\\Programmi\\\\Python\\\\DeepLearning\\\\ProgettoDeep\\\\DsDL\\\\masks\\\\0\\\\prova\"+str(i)+\".tif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento dei percorsi delle immagini per UNET e ResUnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path():\n",
    "    print(\"Load path..\")\n",
    "\n",
    "    # Mi serve per salvare il path delle immagini\n",
    "    image_files = []\n",
    "\n",
    "    # Percorso per prendere tutte le maschere e non le immagini\n",
    "    image_masks = glob('C:/Users/MATTEO/Desktop/Programmi/Python/DeepLearning/ProgettoDeep/Brain_MRI/*/*_mask*')\n",
    "\n",
    "    # Scorro le maschere e rimuovo mask per la corrispondente immagine associata\n",
    "    for i in image_masks:\n",
    "        image_files.append(i.replace('_mask',''))\n",
    "        \n",
    "    return image_files,image_masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento delle immagini + Data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img():\n",
    "    # prendo tutti i path di immagini\n",
    "    img,msk=load_path()\n",
    "    print(\"Load image..\")\n",
    "\n",
    "    # np array per immgini e maschere\n",
    "    imlst=np.zeros((len(img*4),256,256,3),dtype=np.uint8)\n",
    "    mslst=np.zeros((len(msk*4),256,256),dtype=np.bool_)\n",
    "    \n",
    "    # scorro i path e prendo immagini\n",
    "    index=0\n",
    "    j=0\n",
    "    with d2l.try_gpu():\n",
    "        for i in range(len(img)):\n",
    "            index=index+1\n",
    "            print(index)\n",
    "\n",
    "            # carico immagine\n",
    "            im=Image.open(img[i])\n",
    "\n",
    "            # Data augmentation\n",
    "            imRotate=im.rotate(90)\n",
    "            imFlippedLR=im.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "            imFlippedTB=im.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "\n",
    "            # carico maschera\n",
    "            ms=Image.open(msk[i])\n",
    "            msRotate=ms.rotate(90)\n",
    "            msFlippedLR=ms.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "            msFlippedTB=ms.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "            \n",
    "            # metto nell'nparray\n",
    "            imlst[j]=im\n",
    "            mslst[j]=ms\n",
    "            j=j+1\n",
    "            imlst[j]=imRotate\n",
    "            mslst[j]=msRotate\n",
    "            j=j+1\n",
    "            imlst[j]=imFlippedLR\n",
    "            mslst[j]=msFlippedLR\n",
    "            j=j+1\n",
    "            imlst[j]=imFlippedTB\n",
    "            mslst[j]=msFlippedTB\n",
    "            j=j+1\n",
    "    print(j)\n",
    "    return imlst,mslst\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, test, validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    img,msk=load_img()\n",
    "    \n",
    "    print(\"Spleet Data..\")\n",
    "    # Split dataset in train 80% e test 20%\n",
    "    imgTrain,imgTest,mskTrain, mskTest = train_test_split(img,msk, test_size = 0.25)\n",
    "    # Split dal train il validation il 20% \n",
    "    # train, validation = train_test_split(train, test_size = 0.25)\n",
    "\n",
    "    #return train,test,validation\n",
    "    return imgTrain,mskTrain, imgTest, mskTest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train,test,validation=split_data()\n",
    "imgTrain,mskTrain, imgTest, mskTest=split_data()\n",
    "mskTrain=mskTrain.reshape((mskTrain.shape[0],256,256,1))\n",
    "imgTrain=imgTrain.reshape((imgTrain.shape[0],256,256,3))\n",
    "print(imgTrain.shape)\n",
    "print(mskTrain.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Il problema era innanzitutto nello shape delle maskere perche io pensavo che (None,256,256) fosse uguale a (None,256,256,1) invece no.\n",
    " Inoltre nella segmentazione la rete deve generare in output non 1 valore come nelle MLP, ma una immagine di uguale dimensione a quella in input\n",
    " se ho 256,256,3 in input, in output deve avere per funzionare 256,256,1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implementazione Unet per capire la propagazione dell'immagine con i vari re-size dovuti alla convoluzione ed al pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet:\n",
    "    def __init__(self):\n",
    "\n",
    "        inputs = tf.keras.layers.Input((256, 256, 3))\n",
    "\n",
    "        # Encoder\n",
    "        c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        #c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "        c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "        #b1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "        r1 = tf.keras.layers.ReLU()(c1)\n",
    "        p1 = tf.keras.layers.MaxPooling2D((2, 2))(r1)\n",
    "\n",
    "        c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "        #c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "        c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "        #b2 = tf.keras.layers.BatchNormalization()(c2)\n",
    "        r2 = tf.keras.layers.ReLU()(c2)\n",
    "        p2 = tf.keras.layers.MaxPooling2D((2, 2))(r2)\n",
    "        \n",
    "        c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "        #c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "        c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "        #c3 = tf.keras.layers.BatchNormalization()(c3)\n",
    "        r3 = tf.keras.layers.ReLU()(c3)\n",
    "        p3 = tf.keras.layers.MaxPooling2D((2, 2))(r3)\n",
    "        \n",
    "        c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "        #c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "        c4 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "        #b4 = tf.keras.layers.BatchNormalization()(c4)\n",
    "        r4 = tf.keras.layers.ReLU()(c4)\n",
    "        p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(r4)\n",
    "\n",
    "        # Bridge\n",
    "        c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "        c5 = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "        u5 = tf.keras.layers.BatchNormalization()(c5)\n",
    "        u5 = tf.keras.layers.ReLU()(u5)\n",
    "        #c5 = tf.keras.layers.Dropout(0.3)(r5)\n",
    "        \n",
    "\n",
    "        # Decoder \n",
    "        u6 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(u5)\n",
    "        u6 = tf.keras.layers.concatenate([u6, r4])\n",
    "        c6 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "        c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)\n",
    "        #u6 = tf.keras.layers.BatchNormalization()(c6)\n",
    "        u6 = tf.keras.layers.ReLU()(c6)\n",
    "\n",
    "        \n",
    "        u7 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(u6)\n",
    "        u7 = tf.keras.layers.concatenate([u7, r3])\n",
    "        c7 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "        c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)\n",
    "        #u7 = tf.keras.layers.BatchNormalization()(c7)\n",
    "        u7 = tf.keras.layers.ReLU()(c7)\n",
    "\n",
    "        \n",
    "        u8 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(u7)\n",
    "        u8 = tf.keras.layers.concatenate([u8, r2])\n",
    "        c8 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "        c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)\n",
    "        #u8 = tf.keras.layers.BatchNormalization()(c8)\n",
    "        u8 = tf.keras.layers.ReLU()(c8)\n",
    "        \n",
    "        u9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(u8)\n",
    "        u9 = tf.keras.layers.concatenate([u9, r1], axis=3)\n",
    "        c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "        c9 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "        #u9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "        u9 = tf.keras.layers.ReLU()(c9)\n",
    "\n",
    "        # Output\n",
    "        outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(u9)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # For gpu\n",
    "    @jit()\n",
    "    def training(self,xtrain,ytrain):\n",
    "        self.model.fit(xtrain,ytrain,batch_size=32,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with d2l.try_gpu():\n",
    "    UNet=Unet()\n",
    "    UNet.model.summary()\n",
    "    UNet.training(imgTrain,mskTrain)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implementazione della ResUnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUnet:\n",
    "    def __init__(self):\n",
    "        inputs = tf.keras.layers.Input((256,256,3))\n",
    "\n",
    "        # Encoder\n",
    "        a1 = tf.keras.layers.Conv2D(64,1,activation= \"relu\", padding= \"same\")(inputs)\n",
    "\n",
    "        e1 = tf.keras.layers.Conv2D(64,3,activation= \"relu\", padding= \"same\")(inputs)\n",
    "        b1 = tf.keras.layers.BatchNormalization()(e1)\n",
    "        r1 = tf.keras.layers.ReLU()(b1)\n",
    "        e1 = tf.keras.layers.Conv2D(64,3,activation= \"relu\", padding= \"same\")(r1)\n",
    "        e1 = tf.keras.layers.add([e1,a1])\n",
    "        m1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e1)\n",
    "\n",
    "\n",
    "        a2 = tf.keras.layers.Conv2D(128,1,activation= \"relu\", padding= \"same\")(m1)\n",
    "        \n",
    "        b2 = tf.keras.layers.BatchNormalization()(m1)\n",
    "        r2 = tf.keras.layers.ReLU()(b2)\n",
    "        e2 = tf.keras.layers.Conv2D(128,3,activation= \"relu\", padding= \"same\")(r2)\n",
    "        b2 = tf.keras.layers.BatchNormalization()(e2)\n",
    "        r2 = tf.keras.layers.ReLU()(b2)\n",
    "        e2 = tf.keras.layers.Conv2D(128,3,activation= \"relu\", padding= \"same\")(r2)\n",
    "        e2 = tf.keras.layers.add([a2,e2])\n",
    "        m2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e2)\n",
    "\n",
    "\n",
    "        a3 = tf.keras.layers.Conv2D(256,1,activation= \"relu\", padding= \"same\")(m2)\n",
    "\n",
    "        b3 = tf.keras.layers.BatchNormalization()(m2)\n",
    "        r3 = tf.keras.layers.ReLU()(b3)\n",
    "        e3 = tf.keras.layers.Conv2D(256,3,activation= \"relu\", padding= \"same\")(r3)\n",
    "        b3 = tf.keras.layers.BatchNormalization()(e3)\n",
    "        r3 = tf.keras.layers.ReLU()(b3)\n",
    "        e3 = tf.keras.layers.Conv2D(256,3,activation= \"relu\", padding= \"same\")(r3)\n",
    "        e3 = tf.keras.layers.add([a3,e3])\n",
    "        m3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e3)\n",
    "\n",
    "\n",
    "        # Bridge\n",
    "        a4 = tf.keras.layers.Conv2D(512,1,activation= \"relu\", padding= \"same\")(m3)\n",
    "\n",
    "        b4 = tf.keras.layers.BatchNormalization()(m3)\n",
    "        r4 = tf.keras.layers.ReLU()(b4)\n",
    "        e4 = tf.keras.layers.Conv2D(512,3,activation= \"relu\", padding= \"same\")(r4)\n",
    "        b4 = tf.keras.layers.BatchNormalization()(e4)\n",
    "        r4 = tf.keras.layers.ReLU()(b4)\n",
    "        e4 = tf.keras.layers.Conv2D(512,3,activation= \"relu\", padding= \"same\")(r4)\n",
    "        e4 = tf.keras.layers.add([a4,e4])\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        d5 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(e4)\n",
    "        d5 = tf.keras.layers.concatenate([e3,d5])\n",
    "\n",
    "        b6 = tf.keras.layers.BatchNormalization()(d5)\n",
    "        r6 = tf.keras.layers.ReLU()(b6)\n",
    "        d6 = tf.keras.layers.Conv2D(512,3,activation= \"relu\", padding= \"same\")(r6)\n",
    "        b6 = tf.keras.layers.BatchNormalization()(d6)\n",
    "        r6 = tf.keras.layers.ReLU()(b6)\n",
    "        d6 = tf.keras.layers.Conv2D(512,3,activation= \"relu\", padding= \"same\")(r6)\n",
    "        d6 = tf.keras.layers.add([d5,d6])\n",
    "\n",
    "\n",
    "        d7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(d6)\n",
    "        d7 = tf.keras.layers.concatenate([d7,e2])\n",
    "\n",
    "        b8 = tf.keras.layers.BatchNormalization()(d7)\n",
    "        r8 = tf.keras.layers.ReLU()(b8)\n",
    "        d8 = tf.keras.layers.Conv2D(256,3,activation= \"relu\", padding= \"same\")(r8)\n",
    "        b8 = tf.keras.layers.BatchNormalization()(d8)\n",
    "        r8 = tf.keras.layers.ReLU()(b8)\n",
    "        d8 = tf.keras.layers.Conv2D(256,3,activation= \"relu\", padding= \"same\")(r8)\n",
    "        d8 = tf.keras.layers.add([d7,d8])\n",
    "        \n",
    "\n",
    "        d9 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(d8)\n",
    "        d9 = tf.keras.layers.concatenate([d9,e1])\n",
    "        \n",
    "        b10 = tf.keras.layers.BatchNormalization()(d9)\n",
    "        r10 = tf.keras.layers.ReLU()(b10)\n",
    "        d10 = tf.keras.layers.Conv2D(128,3,activation= \"relu\", padding= \"same\")(r10)\n",
    "        b10 = tf.keras.layers.BatchNormalization()(d10)\n",
    "        r10 = tf.keras.layers.ReLU()(b10)\n",
    "        d10 = tf.keras.layers.Conv2D(128,3,activation= \"relu\", padding= \"same\")(d10)\n",
    "        d10 = tf.keras.layers.add([d9,d10])\n",
    "\n",
    "\n",
    "        # Output\n",
    "        outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(d10)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # For gpu\n",
    "    @jit()\n",
    "    def training(self,xtrain,ytrain):\n",
    "        self.model.fit(xtrain,ytrain,batch_size=32,epochs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training e valutazione modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with d2l.try_gpu():\n",
    "    ResUnet=ResUnet()\n",
    "    ResUnet.model.summary()\n",
    "    ResUnet.training(imgTrain,mskTrain)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with d2l.try_gpu():\n",
    "    UNet.model.evaluate(imgTest,mskTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
